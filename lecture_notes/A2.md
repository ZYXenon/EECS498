- SVM

  - `W`: shape=(D,C), `X`: shape=(N,D), `X[i]`: shape=(D,)
  - SVM中loss函数使用的是$L(W)=\frac{1}{N}\sum_{i\neq y_i}L_i(f(x_i, W),y_i)+\lambda R(W)$，其中$f=max(0, s_j-s_{y_i}+1)$自己写一写可以发现每轮对于单个向量，dL/dW就等于：错误标签列直接加x[i]，正确标签列直接减x[i]。注意loss公式中第一项需要除以N，所以每次可以加减`x[i]/num_train`，而正则项不需要除

  - loss中第一项：$$L_i=\sum_{j\neq y_i}\max(0,s_j-s_{y_i}+1)$$，其中$$s_j=W[:,j]^TX[i]$$，$s_{y_i}=W[:,y_i]^TX[i]$

  - torch.tensor的索引需要用tensor，可以使用`torch.arange()`
  - `correct_scores = scores[torch.arange(num_train), y]`如果是`[:,y]`那就会索引每一行的y中列
  - `.unsequeeze(dim)`: 在指定位置增加一个维度（维数为1） 
  - `torch.clamp(input, min, max)`: 返回一个<u>新的</u>张量，其中每个元素都被限制在[min, max]之间
  - **vectorized版本里求梯度怎么实现？？？！！！**
  - `torch.randint(low, high, size, dtype=None, device=None)`: 生成随机整数张量。
    - 包含`low`, 不包含`high`
    - `size`: 张量形状

- Two Layer Network
  - 计算softmax时，如果scores很大，exp计算结果可能会超出浮点数表示范围导致溢出。可以分子分母同时除以e^C，即计算exp(x_i - C)，以此保持数值稳定性。C常用max(x_i)这样可以让exp(x_i - C)结果都小于1
  - 计算loss时最后要除以N。否则loss会随着batch size的变化而变化
  - 交叉熵应该是-ylogy。但如果y是one-hot编码，则可以简化为-log(y_true)
  - cross entropy loss 对 softmax scores求导，结果为$\frac{\partial L}{\partial z_i}=p_i -y_i$，而y是独热码，所以对于正确类别$i=y$，偏导为$p_y-1$，其他类别为$p_i$。注意这里减一后同样需要除以N使其不受batch size影响。原因是减了N个1
  - 如果模型在训练集和验证集中表现差不多，则说明模型欠拟合，模型容量太小，能力弱


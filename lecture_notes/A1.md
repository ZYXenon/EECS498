- `x_train: Tensor of shape (num_train, D1, D2, ...)`这是x_train的shape！取样本数量应该用`x_train.shape[0]`而不是`x_train[0]`，后者是取第一个样本了

- `a ** 2`是将张量中每个元素分别求平方

- 取元素可以用`a[i, j]`

- 会发生广播的情况：
  对于两个张量 `A` 和 `B`，**从最后一维开始**比较：
  - 如果维度相等 ✅
  - 如果某个维度是 1 ✅（会广播）
  - 如果某个维度不存在（短的 tensor 会左侧补 1）✅
  - 否则 ❌ 报错

- 计算两个图像的欧几里得距离时，最好是先将张量展平`train_sample = x_train[i].view(-1)`。因为如果图像维度不一致时可能会发生隐式广播，而展平后如果唯独不一致会直接报错

- `compute_distances_no_loops`函数实现

  - version 1:

    ```python
    train_flat = x_train.view(num_train, 1, -1)
    test_flat = x_test.view(1, num_test, -1)
    dists = torch.sum((test_flat - train_flat) ** 2, dim=2)
    ```

    这个方法虽然没有用for循环，但是会创建一个超级大的中间张量(num_train, num_test, D)，严重增加内存消耗和计算负担
    假设`num_train=5000, num_test=1000`, CIFAR-10图片尺寸是(3, 32, 32) (3072个元素), 那么中间张量内存为`(5000, 1000, 3072) ≈ 15 亿个元素 ≈ 6 GB（float32）`

  - version 2:

    利用$(a-b)^2=a^2+b^2-2ab$，所需的中间张量小很多，而且pytorch的矩阵乘法可以调用底层 BLAS/cuBLAS 加速，更快更省内存

    ```python
    train_flat = x_train.view(num_train, -1)
    test_flat = x_test.view(num_test, -1)
    train_sq = torch.sum(train_flat ** 2, dim=1).view(-1,1)
    test_sq = torch.sum(test_flat ** 2, dim=1).view(1,-1)
    inter_mul = torch.mm(train_flat, test_flat.t())
    dists = train_sq + test_sq - 2 * inter_mul
    ```

    

- `values, indeices = torch.topk(input, k, dim, largest=True, sorted=True)`

  - `input`: 输入的张量，`largest`如果是False那就选最小的k个，`sorted`返回结果是否按值排序

  - ```python
    x = torch.tensor([10, 1, 5, 8, 3])
    values, indices = torch.topk(x, k=3, largest=False)
    
    # 输出
    # values:  tensor([1, 3, 5])
    # indices: tensor([1, 4, 2])
    ```

- `count = torch.bincount(input, minlength=...)`: 统计**非负整数**张量中每个整数出现的次数

  - 对一组标签做 `bincount` 时，如果某些标签 **在样本中没有出现**，`bincount` 默认不会给它们分配空间。这在分类任务中可能导致 **输出维度不足** 或 **索引错误**，这就需要minlength来设置结果的最小长度

  - ```python
    labels = torch.tensor([2, 1, 2, 3, 1])
    counts = torch.bincount(labels)
    
    # 输出: tensor([0, 2, 2, 1])
    # 含义: 标签 0 出现 0 次，1 出现 2 次，2 出现 2 次，3 出现 1 次
    ```

- `idx = torch.argmax(input, dim=None)`: 返回张量中**最大值的索引**（默认在扁平化后的张量上操作）

  - `input`: 输入张量，`dim`: 返回指定维度上最大值的索引，默认按扁平化处理整个张量

- `values, counts = torch.mode(input, dim, keepdim=False)`: 返回输入张量里对应维度上的众数(mode)及其出现次数

- `torch.topk`和`torch.mode`中`dim=0`是找出每列的最大/最多数，`dim=1`是找出每行的

- `torch.chunk(input, chunks, dim=0)`: 将一个张量均匀地切分成若干块。chunks指定要切成几块

  - ```python
    x = torch.tensor([1, 2, 3, 4, 5, 6])
    a, b = torch.chunk(x, chunks=2, dim=0)
    
    # a: tensor([1, 2, 3])
    # b: tensor([4, 5, 6])
    ```

- `torch.cat(tensors, dim=0)`: 用于连接多个张量

  - `tensors`: 一个张量列表（如 `[a, b, c]`），每个张量形状必须**除了拼接的维度外都相同**；
  - `dim`: 沿哪个维度拼接（常用的如 `dim=0` 表示“竖着拼”，`dim=1` 表示“横着拼”）。

- `sorted(iterable, key=None, reverse=False)`
  - `key`指定排序依据（用于计算依据的函数）
  - `reverse` 是否降序排序
  - 返回一个新的列表